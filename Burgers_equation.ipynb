{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burgers' Equation Solution using PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install matplotlib\n",
    "# %pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "  def __init__(self, fc_list, viscosity):\n",
    "    super().__init__()\n",
    "\n",
    "    # for some reason Tanh is the only one that works\n",
    "    self.activation = nn.Tanh()\n",
    "\n",
    "    # viscosity coefficient\n",
    "    self.l = viscosity\n",
    "\n",
    "    self.loss_function = nn.MSELoss()\n",
    "\n",
    "    self.fc_list = fc_list # first, last = input_size, output_size\n",
    "    self.network = nn.ModuleList([nn.Linear(fc_list[i], fc_list[i + 1]) for i in range(len(fc_list) - 1)])\n",
    "\n",
    "    # works better than nothing or nn.init.xavier_normal_\n",
    "    for i in range(len(self.fc_list)-1):\n",
    "      nn.init.kaiming_normal_(self.network[i].weight.data)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = x\n",
    "\n",
    "    for i in range(len(self.fc_list) - 2):\n",
    "      output = self.network[i](output)\n",
    "      output = self.activation(output)\n",
    "\n",
    "    # even though result is between -1 and 1 is looks like it's better not to apply Tanh\n",
    "    return self.network[-1](output)\n",
    "\n",
    "  # t = 0 -> F(x, t) = sin(-pi * x)\n",
    "  def error_initial(self, state):\n",
    "    return self.loss_function(self.forward(state), (-math.pi * state[:, 0].clone().detach()).sin().unsqueeze(1))\n",
    "\n",
    "  # x = ±1 -> F(x, t) = 0\n",
    "  # also F(0, t) = 0 but it's more fair not to include x=0 here\n",
    "  def error_border(self, state):\n",
    "    return self.loss_function(self.forward(state), torch.zeros(state.size(0), 1))\n",
    "\n",
    "  # ∂t*u + u*∂x*u - λ*∂xx*u tells how far is model from truth\n",
    "  def error_pde(self, state):\n",
    "    # it's better not to touch anything under here because it always fails to work normally\n",
    "\n",
    "    state_ = state.clone()\n",
    "    state_.requires_grad = True\n",
    "\n",
    "    u = self.forward(state_)\n",
    "\n",
    "    dx_dt = torch.autograd.grad(\n",
    "      u, state_,\n",
    "      torch.ones([state_.shape[0],1]),\n",
    "      retain_graph=True, create_graph=True\n",
    "    )[0]\n",
    "    dxx_dtt = torch.autograd.grad(\n",
    "      dx_dt, state_,\n",
    "      torch.ones(state_.shape),\n",
    "      create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    u_t = dx_dt[:,[1]]\n",
    "    u_x = dx_dt[:,[0]]\n",
    "    u_xx = dxx_dtt[:,[0]]\n",
    "\n",
    "    residue = u_t + u * u_x - self.l * u_xx\n",
    "    self.last_pde_error = self.loss_function(residue, torch.zeros_like(residue))\n",
    "    return self.loss_function(residue, torch.zeros_like(residue))\n",
    "\n",
    "  def error_total(self, state_beginning, state_border, state_general):\n",
    "    return self.error_initial(state_beginning) + self.error_border(state_border) + self.error_pde(state_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 0.0015\n",
    "SCHEDULER_RATE = 20\n",
    "GAMMA = 0.9999\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "DATASET_SIZE = 50000\n",
    "TRAINING_FRACTION = 0.95\n",
    "\n",
    "FC_LAYERS = [2, 16, 32, 32, 32, 32, 16, 1]\n",
    "\n",
    "VISCOSITY = 0.01 / math.pi\n",
    "\n",
    "solver = PINN(FC_LAYERS, VISCOSITY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TERRIBLE CODE MUST BE REWRITTEN FROM SCRATCH\n",
    "\n",
    "def plot_stats(train_loss: list[float], valid_loss: list[float]):\n",
    "  plt.figure(figsize=(16, 8))\n",
    "\n",
    "  plt.plot(train_loss, label='Training loss')\n",
    "  plt.plot(valid_loss, label='Validation loss')\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.xlabel(\"Epoch\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def plot_fun(x):\n",
    "  u_tensor = solver(x)\n",
    "  u = u_tensor.squeeze().detach().numpy()\n",
    "  return u\n",
    "\n",
    "def plot_t():\n",
    "  x = np.arange(-1, 1.01, 0.01)\n",
    "  x_t = torch.Tensor(x)\n",
    "  inputs1 = torch.cat([x_t.view(201, 1), torch.zeros(x_t.size()).view(201, 1)], dim=1)\n",
    "  inputs2 = torch.cat([x_t.view(201, 1), torch.full_like(x_t, 0.25).view(201, 1)], dim=1)\n",
    "  inputs3 = torch.cat([x_t.view(201, 1), torch.full_like(x_t, 0.5).view(201, 1)], dim=1)\n",
    "  inputs4 = torch.cat([x_t.view(201, 1), torch.full_like(x_t, 0.75).view(201, 1)], dim=1)\n",
    "\n",
    "  u1 = plot_fun(inputs1)\n",
    "  u2 = plot_fun(inputs2)\n",
    "  u3 = plot_fun(inputs3)\n",
    "  u4 = plot_fun(inputs4)\n",
    "\n",
    "  plt.plot(x, u1, marker='', linestyle='solid', color='#fd8a8a')\n",
    "  plt.plot(x, u2, marker='', linestyle='solid', color='#ffcbcb')\n",
    "  plt.plot(x, u3, marker='', linestyle='solid', color='#a8d1d1')\n",
    "  plt.plot(x, u4, marker='', linestyle='solid', color='#9ea1d4')\n",
    "\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('u')\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, num_samples):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_samples = num_samples\n",
    "\n",
    "    num_beginning = int(num_samples)\n",
    "    num_border = int(num_samples)\n",
    "    num_random = int(num_samples)\n",
    "\n",
    "    self.random_x = torch.rand((num_random, 1)) * 2 - 1\n",
    "    self.random_t = torch.rand((num_random, 1))\n",
    "    self.random_all = torch.cat([self.random_x, self.random_t], dim=1)\n",
    "\n",
    "    self.border_x = torch.randint(0, 2, (num_border, 1), dtype=torch.float32) * 2 - 1\n",
    "    self.border_t = torch.rand((num_border, 1))\n",
    "    self.border_all = torch.cat([self.border_x, self.border_t], dim=1)\n",
    "\n",
    "    self.beginning_x = torch.rand((num_beginning, 1)) * 2 - 1\n",
    "    self.beginning_t = torch.zeros(num_beginning, 1)\n",
    "    self.beginning_all = torch.cat([self.beginning_x, self.beginning_t], dim=1)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_samples\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return self.beginning_all[item], self.border_all[item], self.random_all[item]\n",
    "\n",
    "dataset = CustomDataset(DATASET_SIZE)\n",
    "\n",
    "training_dataset, validation_dataset = random_split(dataset,\n",
    "  (int(len(dataset) * TRAINING_FRACTION), len(dataset) -  int(len(dataset) * TRAINING_FRACTION)),\n",
    "  generator=torch.Generator().manual_seed(238)\n",
    ")\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, batch_beginning, batch_border, batch_general, optimizer):\n",
    "  loss = model.error_total(batch_beginning, batch_border, batch_general)\n",
    "  optimizer.zero_grad()\n",
    "  # maybe retain_graph=False\n",
    "  loss.backward(retain_graph=True)\n",
    "  optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def validate_batch(model, batch_beginning, batch_border, batch_general):\n",
    "  loss = model.error_total(batch_beginning, batch_border, batch_general)\n",
    "\n",
    "  return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_loader, validation_loader, optimizer, scheduler):\n",
    "  training_loss_history, validation_loss_history = [], []\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    training_cnt = 0\n",
    "    validation_cnt = 0\n",
    "\n",
    "    for i, (batch_beginning, batch_border, batch_general) in enumerate(training_loader):\n",
    "      training_loss += train_batch(model, batch_beginning, batch_border, batch_general, optimizer)\n",
    "      training_cnt += 1\n",
    "\n",
    "    for i, (batch_beginning, batch_border, batch_general) in enumerate(validation_loader):\n",
    "      validation_loss += validate_batch(model, batch_beginning, batch_border, batch_general)\n",
    "      validation_cnt += 1\n",
    "\n",
    "    training_loss_history.append(training_loss / training_cnt)\n",
    "    validation_loss_history.append(validation_loss / validation_cnt)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      # clear_output(wait=True)\n",
    "      # plot_stats(training_loss_history, validation_loss_history)\n",
    "      # plot_t()\n",
    "      print(\"Epoch {0}, PDE MSE Error: {1}\".format(epoch, model.last_pde_error.item()))\n",
    "\n",
    "    if epoch % SCHEDULER_RATE == 0:\n",
    "      scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(solver.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA)\n",
    "\n",
    "train(solver, training_loader, validation_loader, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_t()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_PINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
